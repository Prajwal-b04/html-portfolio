{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "doc_ids = reuters.fileids()[:10]\n",
        "docs = [\" \".join(reuters.words(fid)) for fid in doc_ids]\n",
        "\n",
        "print(\"Number of documents:\", len(docs))\n",
        "print(\"Sample document:\\n\", docs[0][:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60E_uVJ_rJhj",
        "outputId": "cc9f755d-060a-4f32-f612-553d76a754fc"
      },
      "id": "60E_uVJ_rJhj",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 10\n",
            "Sample document:\n",
            " ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPAN RIFT Mounting trade friction between the U . S . And Japan has raised fears among many of Asia ' s exporting nations that the row could inflict far - reaching economic damage , businessmen and officials said . They told Reuter correspondents in Asian capitals a U . S . Move against Japan might boost protectionist sentiment in the U . S . And lead to curbs on American imports of their products . But some exporters said that while the conflict would \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2a: Word-Document Matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Clean the documents\n",
        "docs_cleaned = []\n",
        "for d in docs:\n",
        "    tokens = nltk.word_tokenize(d.lower())\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
        "    docs_cleaned.append(\" \".join(tokens))\n",
        "\n",
        "# Build TF-IDF Word-Document Matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "WDM = vectorizer.fit_transform(docs_cleaned)\n",
        "\n",
        "wdm_df = pd.DataFrame(WDM.toarray(),\n",
        "                      index=[f\"Doc{i}\" for i in range(len(docs_cleaned))],\n",
        "                      columns=vectorizer.get_feature_names_out())\n",
        "print(\"Word-Document Matrix (first 10 words):\")\n",
        "print(wdm_df.iloc[:, :10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJHsE-MQyMVD",
        "outputId": "db006ac2-fdd2-41f8-8fbe-6108ec443cd7"
      },
      "id": "UJHsE-MQyMVD",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word-Document Matrix (first 10 words):\n",
            "          able  absorbing  according  accounting      act    action     added  \\\n",
            "Doc0  0.034599   0.000000   0.000000    0.034599  0.00000  0.029412  0.000000   \n",
            "Doc1  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc2  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc3  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc4  0.000000   0.000000   0.087781    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc5  0.000000   0.000000   0.000000    0.000000  0.00000  0.125977  0.000000   \n",
            "Doc6  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.039887   \n",
            "Doc7  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc8  0.000000   0.000000   0.000000    0.000000  0.00000  0.000000  0.000000   \n",
            "Doc9  0.000000   0.031805   0.000000    0.000000  0.06361  0.000000  0.027037   \n",
            "\n",
            "      additives  adelaide  advantage  \n",
            "Doc0   0.000000  0.000000   0.034599  \n",
            "Doc1   0.107837  0.000000   0.000000  \n",
            "Doc2   0.000000  0.000000   0.000000  \n",
            "Doc3   0.000000  0.000000   0.000000  \n",
            "Doc4   0.000000  0.000000   0.000000  \n",
            "Doc5   0.000000  0.000000   0.000000  \n",
            "Doc6   0.000000  0.000000   0.000000  \n",
            "Doc7   0.000000  0.000000   0.000000  \n",
            "Doc8   0.000000  0.106038   0.000000  \n",
            "Doc9   0.000000  0.000000   0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2b: Word-Context Matrix\n",
        "import numpy as np\n",
        "\n",
        "window_size = 2\n",
        "tokens = \" \".join(docs_cleaned).split()\n",
        "vocab = list(set(tokens))\n",
        "vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    target = tokens[i]\n",
        "    start = max(0, i - window_size)\n",
        "    end = min(len(tokens), i + window_size + 1)\n",
        "    context = tokens[start:i] + tokens[i+1:end]\n",
        "    for ctx in context:\n",
        "        cooc_matrix[vocab_index[target], vocab_index[ctx]] += 1\n",
        "\n",
        "wcm_df = pd.DataFrame(cooc_matrix, index=vocab, columns=vocab)\n",
        "print(\"Word-Context Matrix (10x10 slice):\")\n",
        "print(wcm_df.iloc[:10, :10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWRp6vkRyQdG",
        "outputId": "7ca318ad-49c2-4bdf-c7f8-f0b92d128659"
      },
      "id": "VWRp6vkRyQdG",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word-Context Matrix (10x10 slice):\n",
            "           trying  mexico  movement  steagall  europe  visit  confident  \\\n",
            "trying        0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "mexico        0.0     0.0       0.0       0.0     1.0    1.0        0.0   \n",
            "movement      0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "steagall      0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "europe        0.0     1.0       0.0       0.0     0.0    1.0        0.0   \n",
            "visit         0.0     1.0       0.0       0.0     1.0    0.0        0.0   \n",
            "confident     0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "possible      0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "stepped       0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "pit           0.0     0.0       0.0       0.0     0.0    0.0        0.0   \n",
            "\n",
            "           possible  stepped  pit  \n",
            "trying          0.0      0.0  0.0  \n",
            "mexico          0.0      0.0  0.0  \n",
            "movement        0.0      0.0  0.0  \n",
            "steagall        0.0      0.0  0.0  \n",
            "europe          0.0      0.0  0.0  \n",
            "visit           0.0      0.0  0.0  \n",
            "confident       0.0      0.0  0.0  \n",
            "possible        0.0      0.0  0.0  \n",
            "stepped         0.0      0.0  0.0  \n",
            "pit             0.0      0.0  0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2c: Pair-Pattern Matrix (using trigrams)\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "bigrams = list(nltk.bigrams(tokens))\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Collect trigram patterns\n",
        "patterns = []\n",
        "for i in range(len(tokens) - 2):\n",
        "    pair = (tokens[i], tokens[i+1])\n",
        "    pattern = tokens[i+2]\n",
        "    patterns.append((pair, pattern))\n",
        "\n",
        "pair_vocab = list(set([p for p, _ in patterns]))\n",
        "pattern_vocab = list(set([pat for _, pat in patterns]))\n",
        "\n",
        "pair_index = {p: i for i, p in enumerate(pair_vocab)}\n",
        "pattern_index = {p: i for i, p in enumerate(pattern_vocab)}\n",
        "\n",
        "ppm = np.zeros((len(pair_vocab), len(pattern_vocab)))\n",
        "for pair, pat in patterns:\n",
        "    ppm[pair_index[pair], pattern_index[pat]] += 1\n",
        "\n",
        "ppm_df = pd.DataFrame(ppm, index=[str(p) for p in pair_vocab], columns=pattern_vocab)\n",
        "print(\"Pair-Pattern Matrix (10x10 slice):\")\n",
        "print(ppm_df.iloc[:10, :10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ4jN7_ZyW2M",
        "outputId": "436f12b2-d77a-4a68-bf1a-d2f5a0217cb6"
      },
      "id": "BQ4jN7_ZyW2M",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pair-Pattern Matrix (10x10 slice):\n",
            "                          trying  mexico  movement  steagall  europe  visit  \\\n",
            "('confident', 'bank')        0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('corp', 'holdings')         0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('japan', 'electric')        0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('half', 'financial')        0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('harahap', 'said')          0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('contacts', 'end')          0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('spokesman', 'leading')     0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('industry', 'miti')         0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('vermin', 'consume')        0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "('oil', 'malaysia')          0.0     0.0       0.0       0.0     0.0    0.0   \n",
            "\n",
            "                          confident  possible  stepped  pit  \n",
            "('confident', 'bank')           0.0       0.0      0.0  0.0  \n",
            "('corp', 'holdings')            0.0       0.0      0.0  0.0  \n",
            "('japan', 'electric')           0.0       0.0      0.0  0.0  \n",
            "('half', 'financial')           0.0       0.0      0.0  0.0  \n",
            "('harahap', 'said')             0.0       0.0      0.0  0.0  \n",
            "('contacts', 'end')             0.0       0.0      0.0  0.0  \n",
            "('spokesman', 'leading')        0.0       0.0      0.0  0.0  \n",
            "('industry', 'miti')            0.0       0.0      0.0  0.0  \n",
            "('vermin', 'consume')           0.0       0.0      0.0  0.0  \n",
            "('oil', 'malaysia')             0.0       0.0      0.0  0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3a: Word Similarity (using cosine similarity on WCM)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "word_sim = cosine_similarity(wcm_df)\n",
        "word_sim_df = pd.DataFrame(word_sim, index=vocab, columns=vocab)\n",
        "print(\"Word Similarity Matrix (10x10 slice):\")\n",
        "print(word_sim_df.iloc[:10, :10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db-Gx3BNyZiT",
        "outputId": "b2b871ed-e03d-43b0-ea50-15d09a0bdb9e"
      },
      "id": "Db-Gx3BNyZiT",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Similarity Matrix (10x10 slice):\n",
            "           trying  mexico  movement  steagall  europe  visit  confident  \\\n",
            "trying       1.00    0.00       0.0  0.000000    0.00   0.00        0.0   \n",
            "mexico       0.00    1.00       0.0  0.000000    0.50   0.25        0.0   \n",
            "movement     0.00    0.00       1.0  0.000000    0.00   0.00        0.0   \n",
            "steagall     0.00    0.00       0.0  1.000000    0.00   0.00        0.0   \n",
            "europe       0.00    0.50       0.0  0.000000    1.00   0.50        0.0   \n",
            "visit        0.00    0.25       0.0  0.000000    0.50   1.00        0.0   \n",
            "confident    0.00    0.00       0.0  0.000000    0.00   0.00        1.0   \n",
            "possible     0.00    0.00       0.0  0.176777    0.00   0.00        0.0   \n",
            "stepped      0.00    0.00       0.0  0.000000    0.25   0.25        0.0   \n",
            "pit          0.25    0.00       0.0  0.000000    0.00   0.00        0.0   \n",
            "\n",
            "           possible  stepped   pit  \n",
            "trying     0.000000     0.00  0.25  \n",
            "mexico     0.000000     0.00  0.00  \n",
            "movement   0.000000     0.00  0.00  \n",
            "steagall   0.176777     0.00  0.00  \n",
            "europe     0.000000     0.25  0.00  \n",
            "visit      0.000000     0.25  0.00  \n",
            "confident  0.000000     0.00  0.00  \n",
            "possible   1.000000     0.00  0.00  \n",
            "stepped    0.000000     1.00  0.00  \n",
            "pit        0.000000     0.00  1.00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3b: Document Similarity (using cosine similarity on WDM)\n",
        "doc_sim = cosine_similarity(WDM)\n",
        "doc_sim_df = pd.DataFrame(doc_sim,\n",
        "                          index=[f\"Doc{i}\" for i in range(len(docs_cleaned))],\n",
        "                          columns=[f\"Doc{i}\" for i in range(len(docs_cleaned))])\n",
        "print(\"Document Similarity Matrix:\")\n",
        "print(doc_sim_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY8Sr9ZiydMD",
        "outputId": "c95a90ee-f183-4024-f65e-5c58571b7718"
      },
      "id": "TY8Sr9ZiydMD",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Similarity Matrix:\n",
            "          Doc0      Doc1      Doc2      Doc3      Doc4      Doc5      Doc6  \\\n",
            "Doc0  1.000000  0.046860  0.152604  0.176819  0.094952  0.083057  0.155819   \n",
            "Doc1  0.046860  1.000000  0.055018  0.134465  0.027890  0.019660  0.040943   \n",
            "Doc2  0.152604  0.055018  1.000000  0.098421  0.065358  0.035638  0.056133   \n",
            "Doc3  0.176819  0.134465  0.098421  1.000000  0.074997  0.018972  0.072322   \n",
            "Doc4  0.094952  0.027890  0.065358  0.074997  1.000000  0.019601  0.174631   \n",
            "Doc5  0.083057  0.019660  0.035638  0.018972  0.019601  1.000000  0.053095   \n",
            "Doc6  0.155819  0.040943  0.056133  0.072322  0.174631  0.053095  1.000000   \n",
            "Doc7  0.055734  0.061395  0.026799  0.057203  0.075203  0.023293  0.056301   \n",
            "Doc8  0.073403  0.066644  0.037367  0.080330  0.022781  0.040575  0.038981   \n",
            "Doc9  0.115423  0.041729  0.057818  0.079580  0.063633  0.049493  0.092785   \n",
            "\n",
            "          Doc7      Doc8      Doc9  \n",
            "Doc0  0.055734  0.073403  0.115423  \n",
            "Doc1  0.061395  0.066644  0.041729  \n",
            "Doc2  0.026799  0.037367  0.057818  \n",
            "Doc3  0.057203  0.080330  0.079580  \n",
            "Doc4  0.075203  0.022781  0.063633  \n",
            "Doc5  0.023293  0.040575  0.049493  \n",
            "Doc6  0.056301  0.038981  0.092785  \n",
            "Doc7  1.000000  0.038575  0.033839  \n",
            "Doc8  0.038575  1.000000  0.052530  \n",
            "Doc9  0.033839  0.052530  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: WDM × WCM (fixed with aligned vocabulary)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Sample corpus\n",
        "docs = [\n",
        "    \"Machine learning is fun and powerful\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"Natural language processing uses machine learning\"\n",
        "]\n",
        "\n",
        "# Preprocess\n",
        "docs_cleaned = [d.lower() for d in docs]\n",
        "\n",
        "# 1) Word-Document Matrix (TF-IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "WDM = vectorizer.fit_transform(docs_cleaned)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "wdm_df = pd.DataFrame(WDM.toarray(),\n",
        "                      index=[f\"Doc{i}\" for i in range(len(docs_cleaned))],\n",
        "                      columns=words)\n",
        "\n",
        "print(\"===== Original Word-Document Matrix (WDM) =====\")\n",
        "print(wdm_df, \"\\n\")\n",
        "\n",
        "# 2) Word-Context Matrix (co-occurrence)\n",
        "tokens = \" \".join(docs_cleaned).split()\n",
        "vocab = list(set(tokens))\n",
        "vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
        "window_size = 2\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    target = tokens[i]\n",
        "    start = max(0, i - window_size)\n",
        "    end = min(len(tokens), i + window_size + 1)\n",
        "    context = tokens[start:i] + tokens[i+1:end]\n",
        "    for ctx in context:\n",
        "        cooc_matrix[vocab_index[target], vocab_index[ctx]] += 1\n",
        "\n",
        "wcm_df = pd.DataFrame(cooc_matrix, index=vocab, columns=vocab)\n",
        "\n",
        "print(\"===== Word-Context Matrix (WCM) [before alignment] =====\")\n",
        "print(\"Shape:\", wcm_df.shape, \"\\n\")\n",
        "\n",
        "# 3) Align vocabularies: keep only WDM words in WCM\n",
        "wcm_aligned = wcm_df.loc[words, words]\n",
        "\n",
        "print(\"===== Word-Context Matrix (WCM) [aligned to WDM vocab] =====\")\n",
        "print(\"Shape:\", wcm_aligned.shape, \"\\n\")\n",
        "\n",
        "# 4) Multiply WDM × WCM_aligned\n",
        "q4_result = WDM @ wcm_aligned.values\n",
        "q4_df = pd.DataFrame(q4_result,\n",
        "                     index=[f\"Doc{i}\" for i in range(len(docs_cleaned))],\n",
        "                     columns=words)\n",
        "\n",
        "print(\"===== Q4: Resultant Word-Document Matrix (WDM × WCM) =====\")\n",
        "print(q4_df, \"\\n\")\n",
        "\n",
        "# 5) Comparison: Original vs New (Doc0, all words)\n",
        "print(\"===== Comparison (Doc0): Original vs Q4 Result =====\")\n",
        "comp_q4 = pd.DataFrame({\n",
        "    \"Original WDM\": wdm_df.iloc[0].values,\n",
        "    \"Q4 Result\": q4_df.iloc[0].values\n",
        "}, index=words)\n",
        "print(comp_q4)\n"
      ],
      "metadata": {
        "id": "836vviFD2zIT",
        "outputId": "b9036a78-0c73-4871-dfd1-a10829edcc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "836vviFD2zIT",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Original Word-Document Matrix (WDM) =====\n",
            "           and      deep       fun        is  language  learning   machine  \\\n",
            "Doc0  0.483591  0.000000  0.483591  0.367784  0.000000  0.285617  0.285617   \n",
            "Doc1  0.000000  0.433452  0.000000  0.329651  0.000000  0.512007  0.256004   \n",
            "Doc2  0.000000  0.000000  0.000000  0.000000  0.461381  0.272499  0.272499   \n",
            "\n",
            "       natural        of  powerful  processing    subset      uses  \n",
            "Doc0  0.000000  0.000000  0.483591    0.000000  0.000000  0.000000  \n",
            "Doc1  0.000000  0.433452  0.000000    0.000000  0.433452  0.000000  \n",
            "Doc2  0.461381  0.000000  0.000000    0.461381  0.000000  0.461381   \n",
            "\n",
            "===== Word-Context Matrix (WCM) [before alignment] =====\n",
            "Shape: (14, 14) \n",
            "\n",
            "===== Word-Context Matrix (WCM) [aligned to WDM vocab] =====\n",
            "Shape: (13, 13) \n",
            "\n",
            "===== Q4: Resultant Word-Document Matrix (WDM × WCM) =====\n",
            "           and      deep       fun        is  language  learning   machine  \\\n",
            "Doc0  1.334966  1.620583  1.620583  1.824033  0.285617  2.559600  1.224634   \n",
            "Doc1  0.763103  0.841658  0.841658  2.146921  0.512007  2.294216  2.732576   \n",
            "Doc2  0.000000  0.272499  0.272499  0.817497  1.656641  2.201639  2.201639   \n",
            "\n",
            "       natural        of  powerful  processing    subset      uses  \n",
            "Doc0  0.571234  0.571234  1.252799    0.285617  0.653400  0.571234  \n",
            "Doc1  0.768011  1.201462  0.945459    0.256004  1.019106  0.768011  \n",
            "Doc2  1.467759  0.544998  0.272499    1.656641  0.272499  1.467759   \n",
            "\n",
            "===== Comparison (Doc0): Original vs Q4 Result =====\n",
            "            Original WDM  Q4 Result\n",
            "and             0.483591   1.334966\n",
            "deep            0.000000   1.620583\n",
            "fun             0.483591   1.620583\n",
            "is              0.367784   1.824033\n",
            "language        0.000000   0.285617\n",
            "learning        0.285617   2.559600\n",
            "machine         0.285617   1.224634\n",
            "natural         0.000000   0.571234\n",
            "of              0.000000   0.571234\n",
            "powerful        0.483591   1.252799\n",
            "processing      0.000000   0.285617\n",
            "subset          0.000000   0.653400\n",
            "uses            0.000000   0.571234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: WDM^T × WDM = Word-Word Matrix\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Same corpus\n",
        "docs = [\n",
        "    \"Machine learning is fun and powerful\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"Natural language processing uses machine learning\"\n",
        "]\n",
        "\n",
        "# Preprocess\n",
        "docs_cleaned = [d.lower() for d in docs]\n",
        "\n",
        "# 1) Word-Document Matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "WDM = vectorizer.fit_transform(docs_cleaned)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "wdm_df = pd.DataFrame(WDM.toarray(),\n",
        "                      index=[f\"Doc{i}\" for i in range(len(docs_cleaned))],\n",
        "                      columns=words)\n",
        "\n",
        "print(\"===== Original Word-Document Matrix (WDM) =====\")\n",
        "print(wdm_df, \"\\n\")\n",
        "\n",
        "# 2) Multiply WDM^T × WDM → Word-Word Matrix\n",
        "q5_result = WDM.T @ WDM\n",
        "q5_df = pd.DataFrame(q5_result.toarray(), index=words, columns=words)\n",
        "\n",
        "print(\"===== Q5: Word-Word Matrix (from WDM^T × WDM) =====\")\n",
        "print(q5_df.iloc[:10, :10], \"\\n\")  # first 10x10 slice\n",
        "\n",
        "# 3) Compare with Original WCM (local co-occurrence)\n",
        "# For fair comparison, we can reuse WCM (from Q4)\n",
        "# NOTE: This requires running Q4 first, because wcm_df is built there.\n",
        "\n",
        "try:\n",
        "    print(\"===== Comparison: WCM vs Q5 Word-Word Matrix (10x10) =====\")\n",
        "    print(\"WCM (local co-occurrence):\")\n",
        "    print(wcm_df.iloc[:10, :10], \"\\n\")\n",
        "    print(\"Q5 Word-Word Matrix (document-based):\")\n",
        "    print(q5_df.iloc[:10, :10])\n",
        "except:\n",
        "    print(\"Run Q4 first to generate WCM for comparison.\")\n"
      ],
      "metadata": {
        "id": "rL0_zRLc3FVz",
        "outputId": "7db1f816-7713-4616-c59a-9f3f88aa48e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rL0_zRLc3FVz",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Original Word-Document Matrix (WDM) =====\n",
            "           and      deep       fun        is  language  learning   machine  \\\n",
            "Doc0  0.483591  0.000000  0.483591  0.367784  0.000000  0.285617  0.285617   \n",
            "Doc1  0.000000  0.433452  0.000000  0.329651  0.000000  0.512007  0.256004   \n",
            "Doc2  0.000000  0.000000  0.000000  0.000000  0.461381  0.272499  0.272499   \n",
            "\n",
            "       natural        of  powerful  processing    subset      uses  \n",
            "Doc0  0.000000  0.000000  0.483591    0.000000  0.000000  0.000000  \n",
            "Doc1  0.000000  0.433452  0.000000    0.000000  0.433452  0.000000  \n",
            "Doc2  0.461381  0.000000  0.000000    0.461381  0.000000  0.461381   \n",
            "\n",
            "===== Q5: Word-Word Matrix (from WDM^T × WDM) =====\n",
            "               and      deep       fun        is  language  learning  \\\n",
            "and       0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "deep      0.000000  0.187880  0.000000  0.142888  0.000000  0.221930   \n",
            "fun       0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "is        0.177857  0.142888  0.177857  0.243935  0.000000  0.273829   \n",
            "language  0.000000  0.000000  0.000000  0.000000  0.212872  0.125726   \n",
            "learning  0.138122  0.221930  0.138122  0.273829  0.125726  0.417984   \n",
            "machine   0.138122  0.110965  0.138122  0.189437  0.125726  0.286908   \n",
            "natural   0.000000  0.000000  0.000000  0.000000  0.212872  0.125726   \n",
            "of        0.000000  0.187880  0.000000  0.142888  0.000000  0.221930   \n",
            "powerful  0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "\n",
            "           machine   natural        of  powerful  \n",
            "and       0.138122  0.000000  0.000000  0.233860  \n",
            "deep      0.110965  0.000000  0.187880  0.000000  \n",
            "fun       0.138122  0.000000  0.000000  0.233860  \n",
            "is        0.189437  0.000000  0.142888  0.177857  \n",
            "language  0.125726  0.212872  0.000000  0.000000  \n",
            "learning  0.286908  0.125726  0.221930  0.138122  \n",
            "machine   0.221370  0.125726  0.110965  0.138122  \n",
            "natural   0.125726  0.212872  0.000000  0.000000  \n",
            "of        0.110965  0.000000  0.187880  0.000000  \n",
            "powerful  0.138122  0.000000  0.000000  0.233860   \n",
            "\n",
            "===== Comparison: WCM vs Q5 Word-Word Matrix (10x10) =====\n",
            "WCM (local co-occurrence):\n",
            "              a  subset  powerful  deep  machine   of  learning  processing  \\\n",
            "a           0.0     1.0       0.0   0.0      0.0  1.0       1.0         0.0   \n",
            "subset      1.0     0.0       0.0   0.0      1.0  1.0       0.0         0.0   \n",
            "powerful    0.0     0.0       0.0   1.0      0.0  0.0       1.0         0.0   \n",
            "deep        0.0     0.0       1.0   0.0      0.0  0.0       1.0         0.0   \n",
            "machine     0.0     1.0       0.0   0.0      0.0  1.0       3.0         1.0   \n",
            "of          1.0     1.0       0.0   0.0      1.0  0.0       1.0         0.0   \n",
            "learning    1.0     0.0       1.0   1.0      3.0  1.0       0.0         0.0   \n",
            "processing  0.0     0.0       0.0   0.0      1.0  0.0       0.0         0.0   \n",
            "is          1.0     1.0       0.0   1.0      1.0  0.0       2.0         0.0   \n",
            "uses        0.0     0.0       0.0   0.0      1.0  0.0       1.0         1.0   \n",
            "\n",
            "             is  uses  \n",
            "a           1.0   0.0  \n",
            "subset      1.0   0.0  \n",
            "powerful    0.0   0.0  \n",
            "deep        1.0   0.0  \n",
            "machine     1.0   1.0  \n",
            "of          0.0   0.0  \n",
            "learning    2.0   1.0  \n",
            "processing  0.0   1.0  \n",
            "is          0.0   0.0  \n",
            "uses        0.0   0.0   \n",
            "\n",
            "Q5 Word-Word Matrix (document-based):\n",
            "               and      deep       fun        is  language  learning  \\\n",
            "and       0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "deep      0.000000  0.187880  0.000000  0.142888  0.000000  0.221930   \n",
            "fun       0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "is        0.177857  0.142888  0.177857  0.243935  0.000000  0.273829   \n",
            "language  0.000000  0.000000  0.000000  0.000000  0.212872  0.125726   \n",
            "learning  0.138122  0.221930  0.138122  0.273829  0.125726  0.417984   \n",
            "machine   0.138122  0.110965  0.138122  0.189437  0.125726  0.286908   \n",
            "natural   0.000000  0.000000  0.000000  0.000000  0.212872  0.125726   \n",
            "of        0.000000  0.187880  0.000000  0.142888  0.000000  0.221930   \n",
            "powerful  0.233860  0.000000  0.233860  0.177857  0.000000  0.138122   \n",
            "\n",
            "           machine   natural        of  powerful  \n",
            "and       0.138122  0.000000  0.000000  0.233860  \n",
            "deep      0.110965  0.000000  0.187880  0.000000  \n",
            "fun       0.138122  0.000000  0.000000  0.233860  \n",
            "is        0.189437  0.000000  0.142888  0.177857  \n",
            "language  0.125726  0.212872  0.000000  0.000000  \n",
            "learning  0.286908  0.125726  0.221930  0.138122  \n",
            "machine   0.221370  0.125726  0.110965  0.138122  \n",
            "natural   0.125726  0.212872  0.000000  0.000000  \n",
            "of        0.110965  0.000000  0.187880  0.000000  \n",
            "powerful  0.138122  0.000000  0.000000  0.233860  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}